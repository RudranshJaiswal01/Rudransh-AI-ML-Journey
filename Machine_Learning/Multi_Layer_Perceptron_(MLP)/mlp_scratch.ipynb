{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "dyh4vdTcUp0N"
      },
      "outputs": [],
      "source": [
        "# importing all the neccessary libraries\n",
        "import torch\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining Multi-Layered Perceptron (MLP) Model\n",
        "class mlp():\n",
        "  def __init__(self, input_size, output_size, hidden_size=(1,), random_seed=None):\n",
        "    if random_seed is not None:\n",
        "      torch.manual_seed(random_seed)\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.output_size = output_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.hidden_layers = len(hidden_size)\n",
        "    self.weights = []\n",
        "    self.bias = []\n",
        "\n",
        "    # Initialising Weights and Bias with requires_grad=True to keep track of derivation/gradients of functions\n",
        "    for i in range(self.hidden_layers):\n",
        "      if i == 0:\n",
        "        self.weights.append(torch.randn(self.input_size, self.hidden_size[i], requires_grad=True))\n",
        "      else:\n",
        "        self.weights.append(torch.randn(self.hidden_size[i-1], self.hidden_size[i], requires_grad=True))\n",
        "\n",
        "      self.bias.append(torch.randn(self.hidden_size[i], requires_grad=True))\n",
        "\n",
        "    self.weights.append(torch.randn(self.hidden_size[-1], self.output_size, requires_grad=True))\n",
        "    self.bias.append(torch.randn(self.output_size, requires_grad=True))\n",
        "\n",
        "  def forward(self, x):\n",
        "    for i in range(self.hidden_layers):\n",
        "      x = self.linear(x, self.weights[i], self.bias[i])\n",
        "      x = self.relu(x)\n",
        "    x = self.linear(x, self.weights[-1], self.bias[-1])\n",
        "    x = self.softmax(x)\n",
        "    return x\n",
        "\n",
        "  # Defining Linear Function\n",
        "  def linear(self, x, w, b):\n",
        "    return x @ w + b\n",
        "\n",
        "  # Defining ReLu Activation Function\n",
        "  def relu(self, x):\n",
        "    return torch.max(x, torch.zeros_like(x))\n",
        "\n",
        "  # Defining Softmax Activation Funtion\n",
        "  def softmax(self, x):\n",
        "    x = x - torch.max(x, dim=-1, keepdim=True)[0]\n",
        "    return torch.exp(x) / torch.exp(x).sum(dim=-1, keepdim=True)\n",
        "\n",
        "  def fit(self, X, y, epochs=100, lr=0.01):\n",
        "    n = X.shape[0]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      # Using SGD to update weights and bias\n",
        "      for i in range(n):\n",
        "        x = X[i]\n",
        "        y_pred = self.forward(x)\n",
        "\n",
        "        loss = -torch.log(y_pred[y[i]]) # Cross-Entropy Loss Funtion\n",
        "\n",
        "        loss.backward() # Backtracking Gradients\n",
        "\n",
        "        with torch.no_grad(): # Stops keeping tracks of gradients\n",
        "          for w in self.weights:\n",
        "            w -= lr * w.grad # Gives gradient w.r.t. the weight\n",
        "            w.grad.zero_() # resets the gradient\n",
        "          for b in self.bias:\n",
        "            b -= lr * b.grad # Gives gradient w.r.t. the bias\n",
        "            b.grad.zero_() # resets the gradient\n",
        "\n",
        "  def predict(self, X):\n",
        "    with torch.no_grad():\n",
        "      y_pred = []\n",
        "      for x in X:\n",
        "        y_pred.append(self.forward(x).argmax().item()) # gives the output with highest probability\n",
        "      return torch.tensor(y_pred, dtype=torch.long)\n",
        "\n",
        "  def predict_proba(self, X):\n",
        "    with torch.no_grad():\n",
        "      y_pred = []\n",
        "      for x in X:\n",
        "        y_pred.append(self.forward(x)) # gives probability of all outputs\n",
        "      return torch.round(torch.stack(y_pred) * 100) / 100 # returning all the probabilty after rounding it off to second place of the decimal"
      ],
      "metadata": {
        "id": "Mrof9l9R154s"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target  # X: features, y: target (class labels)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardise features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "NCG3gVhp7hwQ"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkTE8QVf9zwd",
        "outputId": "d9bec9b3-de8d-48cb-c945-bdd2ea71e261"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((142, 13), (142,), (36, 13), (36,))"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNikQJ9B9_yS",
        "outputId": "4739d817-b775-497d-98f5-8b9615072f10"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 2, 0, 1, 0, 1, 2, 1, 2, 0, 2, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,\n",
              "       1, 2, 2, 2, 1, 1, 1, 0, 0, 1, 2, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)"
      ],
      "metadata": {
        "id": "fId60Bon5ud4"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)"
      ],
      "metadata": {
        "id": "dmOXiAuM50NL"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_wine = mlp(input_size=13, hidden_size=(8,4), output_size=3, random_seed=0) # Initialising model\n",
        "model_wine.fit(X_train, y_train, epochs=1000, lr=0.005) # Training model"
      ],
      "metadata": {
        "id": "Jd_3XfA5-oJW"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model_wine.predict(X_test) # Predicting on test data\n",
        "y_pred, y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DykrBEJr_YK7",
        "outputId": "d2f0ca52-2b3b-49d3-b96f-28eeeccfabd1"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0, 0, 2, 0, 1, 0, 1, 2, 1, 2, 0, 2, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 2,\n",
              "         2, 2, 1, 1, 1, 0, 0, 1, 2, 0, 0, 0]),\n",
              " tensor([0, 0, 2, 0, 1, 0, 1, 2, 1, 2, 0, 2, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 2,\n",
              "         2, 2, 1, 1, 1, 0, 0, 1, 2, 0, 0, 0]))"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(y_pred != y_test).sum() # counting wrong predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmMTKrYZA6Lv",
        "outputId": "20e182b7-b8e7-4e5b-c000-d96d3e30d1be"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0)"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_proba = model_wine.predict_proba(X_test) # Predicting probabilities on test data\n",
        "y_pred_proba"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yf5Fyj3qV8e_",
        "outputId": "ac708aa5-63c2-4557-a5ce-731020fb187b"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000],\n",
              "        [1.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 0.0000, 1.0000],\n",
              "        [1.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 1.0000, 0.0000],\n",
              "        [1.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 1.0000, 0.0000],\n",
              "        [0.0000, 0.0000, 1.0000],\n",
              "        [0.0100, 0.9900, 0.0000],\n",
              "        [0.0000, 0.0000, 1.0000],\n",
              "        [1.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 0.0000, 1.0000],\n",
              "        [0.8900, 0.1100, 0.0000],\n",
              "        [0.0000, 1.0000, 0.0000],\n",
              "        [1.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 1.0000, 0.0000],\n",
              "        [0.0000, 1.0000, 0.0000],\n",
              "        [0.0000, 1.0000, 0.0000],\n",
              "        [1.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 1.0000, 0.0000],\n",
              "        [1.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 1.0000, 0.0000],\n",
              "        [0.0000, 1.0000, 0.0000],\n",
              "        [0.0000, 0.0000, 1.0000],\n",
              "        [0.0000, 0.0000, 1.0000],\n",
              "        [0.0000, 0.0000, 1.0000],\n",
              "        [0.0000, 1.0000, 0.0000],\n",
              "        [0.0000, 1.0000, 0.0000],\n",
              "        [0.0000, 1.0000, 0.0000],\n",
              "        [1.0000, 0.0000, 0.0000],\n",
              "        [1.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 1.0000, 0.0000],\n",
              "        [0.0000, 0.0000, 1.0000],\n",
              "        [1.0000, 0.0000, 0.0000],\n",
              "        [1.0000, 0.0000, 0.0000],\n",
              "        [1.0000, 0.0000, 0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_proba.sum(dim=-1) # checking if the probabilties add up to one or not"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFpHlVxJV_Ns",
        "outputId": "9de4f536-b242-4147-8ee0-d437c9fc33e9"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    }
  ]
}